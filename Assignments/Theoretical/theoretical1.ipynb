{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "## Introduction\n",
    "\n",
    "In this notebook we are going to study gradient descent and in this case applied to logistic regression.\n",
    "Logistic regression (LR) is a statistical method for analysing datasets where there are one of more independent variables that determine the outcome. The outcome is a dichotomous, meaning there are only two possible outcomes (1 / 0, Yes / No, True / False). For instance, if you want to predict the sex of a person from age ($x_1$) and income ($x_2$), the logistic regression model would be\n",
    "\n",
    "$$ h(x) = b_0 + \\theta_0x_1 + \\theta_1 x_2 $$\n",
    "\n",
    "where $h(x)$ is the outcome varibale, $b_0$ the bias and $\\theta$ the weights. The goal is ultimately to tune these parameters with respect to the obeserved data ($x_1$,$x_2$)\n",
    "\n",
    "LR estimates a probability (between 0 and 100%) but $h(x)$ gives values in $(-\\infty, +\\infty)$. We need to \"squish\" $h(x)$ to restrict it to a suitable range. LR commonly uses the logistic function (a.k.a. sigmoid function) to compute probabilities: \n",
    "\n",
    "$$ \\sigma(h(x)) = \\frac{1}{1+e^{-h(x)}} $$\n",
    "\n",
    "It is possible to threshold the logistic function (values between 0-1), and values below 0.5 will be counted as the prediction of class 0 and values larger than 0.5 results in the prediction of class 1.\n",
    "\n",
    "The full logistic regression model is then:\n",
    "\n",
    "$$ z(x) = \\sigma(h(x)) = \\frac{1}{1+e^{-(b_0 + \\theta_0x_1 + \\theta_1 x_2)}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ready, steady, code! ðŸš€\n",
    "\n",
    "Let's start with loading some data, scikit-learn comes with a couple of toy datasets and we are going to use the \"iris\" dataset where the goal is to classify which type of flower based on a set of features consisting of sepal length (cm), sepal width (cm), petal length (cm), petal width (cm). To begin with we consider only two of those features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T07:50:41.692245Z",
     "start_time": "2021-01-19T07:50:41.685547Z"
    }
   },
   "outputs": [],
   "source": [
    "# import stuff that we need\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T07:52:28.766916Z",
     "start_time": "2021-01-19T07:52:28.757691Z"
    }
   },
   "outputs": [],
   "source": [
    "assert np.__version__ == \"1.19.4\", \"Looks like you don't have the same version of numpy as us!\"\n",
    "assert mpl.__version__ == \"3.3.3\", \"Looks like you don't have the same version of matplotlib as us!\"\n",
    "assert sklearn.__version__ == \"0.24.0\", \"Looks like you don't have the same version of sklearn as us!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T07:52:33.179008Z",
     "start_time": "2021-01-19T07:52:33.151459Z"
    }
   },
   "outputs": [],
   "source": [
    "data = ds.load_iris()\n",
    "\n",
    "selected_features_idx = [0,1] #'sepal length (cm)', 'sepal width (cm)'\n",
    "selected_targets = [0,1] #'setosa' 'versicolor'\n",
    "\n",
    "idx = np.array([x in selected_targets for x in data.target])\n",
    "x = data.data[:,selected_features_idx][idx]\n",
    "y = data.target[idx]\n",
    "y[y > 1] = 1 # Reset labels greater than 1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T07:52:46.508713Z",
     "start_time": "2021-01-19T07:52:46.172908Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for label in np.unique(y):\n",
    "    plt.scatter(x[:,0][y==label],x[:,1][y==label], label = data.target_names[label])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a function that predicts the logistic regression model and make predictions. This function takes a measurement, the current bias and the weights as input.\n",
    "\n",
    "$$ z(x) = \\frac{1}{1+e^{-(b_0 + \\theta_0x_1 + \\theta_1 x_2)}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_x(x, bias, weights):\n",
    "    \"\"\" param x: vector containing measurements. x =Â [x1, x2]\n",
    "        param bias: single value\n",
    "        param weight: vector containing model weights. weights= [w1,w2]\n",
    "        \n",
    "        return: value of logistic regression model for defined x, bias and weights\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try it with some random weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = np.random.normal()\n",
    "weights = np.random.normal(size = len(x[0]))\n",
    "\n",
    "predicted = []\n",
    "for i in range(len(x)):\n",
    "    yhat = z_x(x[i], bias, weights)\n",
    "    predicted.append(round(yhat))\n",
    "\n",
    "print('Accuracy: ', np.sum(np.equal(y,predicted)) / len(predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets plot the decision boundary between the points for this set of weights. The decision boundary is found by setting z(x) = 0 which gives:\n",
    "\n",
    "$$ x_2 = -\\frac{b_0 + \\theta_0x_1}{\\theta_1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = x[:,0]\n",
    "y_values = - (bias + weights[0]*x_values) / weights[1]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x[:,0][y==0],x[:,1][y==0], label = '0')\n",
    "plt.scatter(x[:,0][y==1],x[:,1][y==1], label = '2')\n",
    "plt.plot(x_values, y_values, label='Decision Boundary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very good (or did you get lucky with the weights?). Try rerunning it a couple of times to see if you can randomly find a better set of weights that improves the accuracy.\n",
    "\n",
    "Now, a better way of finding the optimal weights is the gradient descent method. Gradient descent is an iterative process of minimizing a function by following the gradients of a pre defined cost function. This is useful for updating and tuning the parameters of our logistic regression model. The updates are defined as:\n",
    "\n",
    "$$ \\theta_j \\leftarrow \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial\\theta_j} $$\n",
    "\n",
    "and similarily for the bias term \n",
    "\n",
    "$$ \\theta_j \\leftarrow b_0 - \\alpha \\frac{\\partial J(b_0)}{\\partial b_o} $$\n",
    "\n",
    "where $\\alpha$ is a user specified learning rate, controlling the step size of each update and $J(\\theta)$ is the cost function. Hence, to minimize the cost function, we move in the direction opposite to the gradient.\n",
    "\n",
    "We need to define a cost function and for binary classifications Binary-Cross-Entropy Loss Function is a good choise. Binary-Cross-Entropy is defined as \n",
    "\n",
    "$$\n",
    "J(\\theta)=\n",
    "\\begin{cases}\n",
    "-log(g(x)),& \\text{if } y = 1\\\\\n",
    "-log(1 - g(x)),& \\text{if } y = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where y is the target class. The Binary-Cross-Entropy tells us that if the target is 1 and we predict 0, then we will get a large error ($-log(0) = \\infty$) and vice verca ($-log(1 - 1) = -log(0) = \\infty$).\n",
    "\n",
    "The two functions can be combined into one as:\n",
    "$$ J(\\theta)= -y\\cdot log(z(x)) - (1-y)\\cdot log(1-z(x))$$\n",
    "\n",
    "For gradient descent we need the derivative of this cost function with respect to the weights $\\frac{\\partial J(\\theta)}{\\partial\\theta_j}$. We can get this with the chain rule:\n",
    "\n",
    "$$Â \\frac{\\partial J(\\theta)}{\\partial\\theta_j} = \\frac{\\partial J(\\theta)}{\\partial z(x)} \\cdot \\frac{\\partial z(x)}{\\partial h(x)} \\cdot \\frac{\\partial h(x)}{\\partial \\theta}$$\n",
    "\n",
    "Where the three derivatives result in:\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial z(x)} =  -(\\frac{y}{z(x)} - \\frac{(1-y)}{(1-z(x)}$$\n",
    "$$\\frac{\\partial z(x)}{\\partial h(x)} = z(x)\\cdot(1-z(x))$$\n",
    "$$\\frac{\\partial h(x)}{\\partial \\theta} = Â x$$\n",
    "\n",
    "which combined gives \n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial\\theta} = x\\cdot(z(x)-y) $$\n",
    "\n",
    "for the bias term the derivative is similar but it's not dependent on x since $\\frac{\\partial h(x)}{\\partial b_0} = Â 1$\n",
    "\n",
    "$$ \\frac{\\partial J(b_0)}{\\partial b_0} = z(x)-y $$\n",
    "\n",
    "The full algorithm is:\n",
    "1. initialize the weights randomly \n",
    "2. Calculate the gradients of cost function w.r.t parameters\n",
    "3. Update the weights by $ \\theta_j \\leftarrow \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j}J(\\theta) $\n",
    "4. Update the bias by $ b_0 \\leftarrow b_0 - \\alpha \\frac{\\partial}{\\partial b_0}J(b_0) $\n",
    "5. repeat until value of cost function does not change or to a pre-defined number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function for the cost and one for its derivative with respect to the weights and one with respect to the bias. Note that the derivative function will return the number of values corresponing the the number of weights that you have    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(y, x, bias, weights):\n",
    "    \"\"\" param y: Ground truth label for measurements\n",
    "        param x: vector containing measurements. x =Â [x1, x2]\n",
    "        param bias: single value\n",
    "        param weight: vector containing model weights. weights= [w1,w2]\n",
    "    \n",
    "        return: value of the cost function. In this case BCE\n",
    "    \"\"\"\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_weights(y, x, bias, weights):\n",
    "    \"\"\" param y: Ground truth label for measurements\n",
    "        param x: vector containing measurements. x =Â [x1, x2]\n",
    "        param bias: single value\n",
    "        param weight: vector containing model weights. weights= [w1,w2]\n",
    "    \n",
    "        return: derivative of cost function with respect to the weights, dw = [dw1, dw2]\n",
    "    \"\"\"\n",
    "    return cost_deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_bias(y, x, bias, weights):\n",
    "    \"\"\" param y: Ground truth label for measurements\n",
    "        param x: vector containing measurements. x =Â [x1, x2]\n",
    "        param bias: single value\n",
    "        param weight: vector containing model weights. weights= [w1,w2]\n",
    "    \n",
    "        return: derivative of cost function with respect to the weights, dw = [dw1, dw2]\n",
    "    \"\"\"\n",
    "    return cost_deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets fit the logistic regression model with gradient descent. Gradient descent works by, at each iteration, average the total cost and the derivatives on over the full training set like:\n",
    "\n",
    "$$  J(\\theta)=\\frac{1}{M}\\sum_i^M -y\\cdot log(z(x_i)) - (1-y_i)\\cdot log(1-z(x_i))$$\n",
    "$$  \\frac{\\partial J(\\theta)}{\\partial\\theta_j} = \\frac{1}{M}\\sum_i^M x_i\\cdot(z(x_i)-y_i) $$\n",
    "\n",
    "Now we have everything we need to do gradient descent for logistic regression, remember the algorithm for gradient descent is:\n",
    "1. initialize the weights randomly \n",
    "2. Calculate the gradients of cost function w.r.t parameters\n",
    "3. Update the weights by $ \\theta_j \\leftarrow \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j}J(\\theta) $\n",
    "4. Update the bias by $ b_0 \\leftarrow b_0 - \\alpha \\frac{\\partial}{\\partial b_0}J(b_0) $\n",
    "5. repeat until value of cost function does not change or to a pre-defined number of iterations\n",
    "\n",
    "Implement gradient descent for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = ... #<-- specify learning rate\n",
    "\n",
    "#initialize weights and bias as random\n",
    "bias = np.random.normal()\n",
    "weights = np.random.normal(size = len(x[0]))\n",
    "\n",
    "\n",
    "number_of_iterations = ... #<-- number of iterations to perform gradient descent\n",
    "\n",
    "#Loop through training data and update the weights at each iteration\n",
    "\n",
    "for it in range(number_of_iterations):\n",
    "# .. Code for gradient descent for logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "for i in range(len(x)):\n",
    "    yhat = z_x(x[i], bias, weights)\n",
    "    predicted.append(round(yhat))\n",
    "\n",
    "print('Accuracy: ', np.sum(np.equal(y,predicted)) / len(predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the decision boundary for the new weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = x[:,0]\n",
    "y_values = - (bias + weights[0]*x_values) / weights[1]\n",
    "\n",
    "plt.figure()\n",
    "for label in np.unique(y):\n",
    "    plt.scatter(x[:,0][y==label],x[:,1][y==label], label = data.target_names[label])\n",
    "plt.plot(x_values, y_values, label='Decision Boundary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
